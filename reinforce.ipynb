{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python37464bitbasecondabe26d826abfc4b47b9227d5c807c1cad",
      "display_name": "Python 3.7.4 64-bit ('base': conda)"
    },
    "colab": {
      "name": "Copy of Copy of q2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT7l0VLHqhZR"
      },
      "source": [
        "import sys\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from datetime import datetime\n",
        "import gym\n",
        "import matplotlib\n",
        "from tqdm import tqdm\n",
        "matplotlib.use('Agg')\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOgQS2PxxEhk"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdj_348erQ43"
      },
      "source": [
        "!pip install Box2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m47Efp7nHiYg"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/My\\ Drive/CMU/rl/hw3\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR4JiOuqqhZZ"
      },
      "source": [
        "class BanditEnv(gym.Env):\n",
        "    '''\n",
        "    Toy env to test your implementation\n",
        "    The state is fixed (bandit setup)\n",
        "    Note that the action takes integer values\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.action_space = gym.spaces.Discrete(10)\n",
        "        self.observation_space = gym.spaces.Box(low=np.array([-1]), high=np.array([1]), dtype=np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        return np.array([0])\n",
        "\n",
        "    def step(self, action):\n",
        "        assert int(action) in self.action_space\n",
        "\n",
        "        done = True\n",
        "        s = np.array([0])\n",
        "        r = -(action - 7)**2\n",
        "        info = {}\n",
        "        return s, r, done, info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17tBXfySqhZe"
      },
      "source": [
        "def generate_episode(env, model, max_length=3000, render=False, train=True):\n",
        "    rewards = []\n",
        "    states = []\n",
        "    # actions = []\n",
        "    log_probs = []\n",
        "    state = env.reset()\n",
        "    for t in range(max_length):\n",
        "        # breakpoint()\n",
        "        state = torch.tensor(np.expand_dims(state, axis=0), dtype=torch.float, device=device) # shape (1, state_size)\n",
        "        probs = model(state)\n",
        "        # probs = F.softmax(out, dim=1)\n",
        "        # action = np.random.choice(range(model.output_size), p=probs.squeeze().cpu().detach().numpy())\n",
        "\n",
        "        dist = Categorical(probs=probs)\n",
        "        action = dist.sample()\n",
        "        state, reward, done, _ = env.step(action.item())\n",
        "        # state, reward, done, _ = env.step(action)\n",
        "        if render:\n",
        "            env.render()\n",
        "        \n",
        "        log_prob = dist.log_prob(action)\n",
        "        # log_prob = F.log_softmax(out, dim=1)[:, action]\n",
        "        \n",
        "        rewards.append(reward)\n",
        "        if train:\n",
        "            # actions.append(action)\n",
        "            # states.append(state)\n",
        "            log_probs.append(log_prob)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    return states, None, rewards, log_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ9YNCS2qhZj"
      },
      "source": [
        "epsilon = 1e-8\n",
        "\n",
        "def get_g(rewards, gamma=0.99, normalize=False):\n",
        "    g = []\n",
        "    T = len(rewards)\n",
        "    for t in range(T):\n",
        "        g_t = np.power(np.full(T-t, fill_value=gamma), range(T-t)) @ rewards[-(T-t):]\n",
        "        g.append(g_t)\n",
        "    g = np.array(g)\n",
        "\n",
        "    if normalize:\n",
        "        g = (g - np.mean(g)) / (np.std(g) + epsilon)\n",
        "    \n",
        "    return g\n",
        "\n",
        "def update_running_means(running_means, counts, rewards):\n",
        "    running_means[:len(rewards)] = (running_means[:len(rewards)] * counts[:len(rewards)] + np.array(rewards)) / (counts[:len(rewards)] + 1)\n",
        "    counts[:len(rewards)] += 1 # update counts only for those as part of trajectory\n",
        "    return running_means, counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhwwUMQvqhZn"
      },
      "source": [
        "def train(model, env, optimizer, scheduler, num_episodes, run_tests=True, k=100, gamma=0.99, normalize=False, baseline=None):\n",
        "    rewards = []\n",
        "    lengths = []\n",
        "    losses = []\n",
        "    print_interval = num_episodes // 100\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for e in tqdm(range(1, num_episodes+1), position=0, leave=True):\n",
        "        optimizer.zero_grad()\n",
        "        _, _, reward, log_prob = generate_episode(env, model, max_length=3000, render=False)\n",
        "        g = get_g(reward, gamma, normalize=normalize)\n",
        "\n",
        "        if baseline == \"time-dependent\":\n",
        "            model.running_means, model.counts = update_running_means(model.running_means, model.counts, reward)\n",
        "            g -= model.running_means[:len(g)]\n",
        "        g = torch.tensor(g, dtype=torch.float, device=device)\n",
        "        loss = - (g * torch.cat(log_prob)).mean()\n",
        "        # import pdb; pdb.set_trace()\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_reward = np.sum(reward)\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "\n",
        "        length = len(reward)\n",
        "        lengths.append(length)\n",
        "\n",
        "        # run test every k episodes (based on model's total count)\n",
        "        if run_tests and model.num_episodes % k == 0:\n",
        "            mean_reward, stdev_reward = test(model, env, num_episodes=100)\n",
        "\n",
        "            # for LL only, print messages for every k (100) iters\n",
        "            if model.env_name == \"LunarLander-v2\":\n",
        "                print(f\"\\nEpisode #{e}. Total episodes:{model.num_episodes}\")\n",
        "                print(f\"current loss:{loss.item()}\")\n",
        "                print(f\"mean_loss: {np.mean(losses)}, reward (train): {total_reward}, mean_reward (train): {np.mean(rewards)}\")\n",
        "                print(f\"mean_reward (eval):{mean_reward}, stdev_reward (eval):{stdev_reward}\")\n",
        "                print(f\"avg trj len:{np.mean(lengths)}\")\n",
        "                print(\"--------------------------\")\n",
        "                rewards = []\n",
        "                losses = []\n",
        "\n",
        "            if scheduler:\n",
        "                scheduler.step(mean_reward)\n",
        "            \n",
        "            torch.save(model.state_dict(), f\"models/2_{model.env_name}_{model.num_episodes}_{datetime.now().isoformat('_').replace(':', '_')}.model\")\n",
        "\n",
        "        \n",
        "        # for bandit, only print messages for every 5% of training progress\n",
        "        if e % print_interval == 0 and not ( run_tests and model.num_episodes % k == 0):\n",
        "            print(f\"\\nEpisode #{e}. Total episodes:{model.num_episodes}\")\n",
        "            print(f\"mean_loss: {np.mean(losses)}, reward (train): {total_reward}, mean_reward (train): {np.mean(rewards)}\")\n",
        "            print(\"--------------------------\")\n",
        "\n",
        "        model.num_episodes += 1\n",
        "\n",
        "    print(f\"Finished training. Total episodes trained: {model.num_episodes}\")\n",
        "\n",
        "def test(model, env, num_episodes=100):\n",
        "    model.eval()\n",
        "    rewards = []\n",
        "    with torch.no_grad():\n",
        "        for e in range(1, num_episodes+1):\n",
        "            _, _, reward, _ = generate_episode(env, model, max_length=3000, train=False)\n",
        "            rewards.append(np.sum(reward))\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    mean_reward, stdev_reward = np.mean(rewards), np.std(rewards)\n",
        "    model.mean_rewards.append(mean_reward)\n",
        "    model.stdev_rewards.append(stdev_reward)\n",
        "    return mean_reward, stdev_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lwUkiZcqhZw"
      },
      "source": [
        "def plot_training(model, k=100):\n",
        "    mean_rewards, stdev_rewards = model.mean_rewards, model.stdev_rewards\n",
        "\n",
        "    above = []\n",
        "    below = []\n",
        "\n",
        "    # add and subtract the stdev to current mean\n",
        "    for mu, sigma in zip(mean_rewards, stdev_rewards):\n",
        "         above.append(mu + sigma)\n",
        "         below.append(mu - sigma)\n",
        "    # breakpoint()\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(np.arange(len(mean_rewards)) * k, mean_rewards, label=\"mean_rewards\")\n",
        "    plt.fill_between(np.arange(len(mean_rewards)) * k, above, below, alpha=0.5, color='y', label=\"stdev_reward\")\n",
        "    plt.title(f\"Mean/Stdev Eval Reward for {model.env_name}, REINFORCE, k={k}, trained: {model.num_episodes} episodes\")\n",
        "    plt.ylabel(f\"Reward\")\n",
        "    plt.xlabel(f\"# Episodes of training (eval every {k})\")\n",
        "    plt.legend()\n",
        "    plt.savefig(f\"p2_{model.env_name}_{datetime.now().isoformat('_').replace(':', '_')}.png\", dpi=300)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Opspo7UDvKno"
      },
      "source": [
        "## Lunar Lander"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOuOwj08vCzR"
      },
      "source": [
        "class ModelTwo(nn.Module):\n",
        "    def __init__(self, input_size, output_size, env_name):\n",
        "        super(ModelTwo, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.num_episodes = 1\n",
        "        self.env_name = env_name\n",
        "        self.mean_rewards = []\n",
        "        self.stdev_rewards = []\n",
        "\n",
        "        self.running_means = np.zeros(3000)\n",
        "        self.counts = np.zeros(3000)\n",
        "\n",
        "        layers = [\n",
        "            nn.Linear(input_size, 16), # layer 1\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 16), # layer 2\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(16, 16), # layer 3\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, output_size), # out layer\n",
        "        ]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.softmax(self.layers(x), dim=1)\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.layers:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                # nn.init.xavier_uniform_(m.weight, gain=0.3)\n",
        "                scale = 1.0\n",
        "                n = np.mean(m.weight.shape)\n",
        "                alpha = np.sqrt(3 * scale / n)\n",
        "                nn.init.uniform_(m.weight, a=-alpha, b=alpha)\n",
        "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                # nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "                nn.init.zeros_(m.bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUwIwUQyFkNz"
      },
      "source": [
        "env = gym.make('LunarLander-v2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G1G0AscsTA5"
      },
      "source": [
        "model = ModelTwo(input_size=env.observation_space.shape[0], output_size=env.action_space.n, env_name=\"LunarLander-v2\")\n",
        "# model.load_state_dict(torch.load(\"models/LunarLander-v2_6000_2020-10-29_19_13_06.266696.model\"))\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq2x8B9J4961"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "# optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-5)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
        "# optimizer = adabound.AdaBound(model.parameters(), lr=1e-5, final_lr=0.1)\n",
        "scheduler = None\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, threshold=10, patience=2, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57YQwbfUqhaH"
      },
      "source": [
        "k=100\n",
        "train(model, env, optimizer, scheduler, num_episodes=1000, run_tests=True, k=k, gamma=0.99, normalize=True, baseline=\"time-dependent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4VFN78kuam2"
      },
      "source": [
        "plot_training(model, k=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OzRrmB4nraF"
      },
      "source": [
        "for param_group in optimizer.param_groups:\n",
        "    param_group['lr']=1e-3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Re5P2XKHllL"
      },
      "source": [
        "optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRz4Isa0exHa"
      },
      "source": [
        "stats = {\"mean_rewards\":model.mean_rewards,\n",
        " \"stdev_rewards\":model.stdev_rewards,\n",
        " \"num_episodes\":model.num_episodes}\n",
        "torch.save(model.state_dict(), f\"models/2_{model.env_name}_{model.num_episodes}_{datetime.now().isoformat('_').replace(':', '_')}.model\")\n",
        "torch.save(stats, f\"models/stats_2_{model.env_name}_{model.num_episodes}_{datetime.now().isoformat('_').replace(':', '_')}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWuLjpi1-DCv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}